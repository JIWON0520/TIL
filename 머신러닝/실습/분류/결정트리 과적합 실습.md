## 결정트리 과적합 예제

```python
from sklearn.datasets import make_classification
import matplotlib.pyplot as plt
%matplotlib inline

plt.title("3 Class values with 2 Features Sample data creation")

#2차원 시각화를 위해서 피저는 2개, 클래스는 3가지 유형의 분류 샘플 데이터 생성
X_features,y_labels=make_classification(n_features=2,n_redundant=0,n_informative=2,n_classes=3,n_clusters_per_class=1,random_state=0)

#그래프 형태로 2개의 피처로 2차원 좌표 시각화, 각 클래스 값은 다른 색깔로 표시됨
plt.scatter(X_features[:,0],X_features[:,1],marker='o',c=y_labels,s=24,edgecolor='k')
```

![결과1](https://user-images.githubusercontent.com/77263283/124779267-9a253300-df7c-11eb-81da-5d34dc1ac2d2.png)

make_classification() 호출로 2개의 피쳐와 3개의 레이블을 가지는 임의의 데이터 세트 생성후 그래프로 표시했다.

각 피처가 X,Y축으로 나열된 그래프이며, 3개의 클래스 값 구분은 색깔로 되어 있다.

결정트리에 별다른 제약이 없도록 학습하고 결정 기준을 시각화 해보자.

```python
import numpy as np
from sklearn.tree import DecisionTreeClassifier

# Classifier의 Decision Boundary를 시각화 하는 함수
def visualize_boundary(model, X, y):
    fig,ax = plt.subplots()
    
    # 학습 데이타 scatter plot으로 나타내기
    ax.scatter(X[:, 0], X[:, 1], c=y, s=25, cmap='rainbow', edgecolor='k',
               clim=(y.min(), y.max()), zorder=3)
    ax.axis('tight')
    ax.axis('off')
    xlim_start , xlim_end = ax.get_xlim()
    ylim_start , ylim_end = ax.get_ylim()
    
    # 호출 파라미터로 들어온 training 데이타로 model 학습 . 
    model.fit(X, y)
    # meshgrid 형태인 모든 좌표값으로 예측 수행. 
    xx, yy = np.meshgrid(np.linspace(xlim_start,xlim_end, num=200),np.linspace(ylim_start,ylim_end, num=200))
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)
    
    # contourf() 를 이용하여 class boundary 를 visualization 수행. 
    n_classes = len(np.unique(y))
    contours = ax.contourf(xx, yy, Z, alpha=0.3,
                           levels=np.arange(n_classes + 1) - 0.5,
                           cmap='rainbow', clim=(y.min(), y.max()),
                           zorder=1)

#특정한 트리 생성 제약 없는 결정 트리의 학습과 결정 경계 시각화
dt_clf=DecisionTreeClassifier().fit(X_features,y_labels)
visualize_boundary(dt_clf,X_features,y_labels)
```

![결과2](https://user-images.githubusercontent.com/77263283/124779290-a01b1400-df7c-11eb-9885-c2ab00f86f7f.png)

별다 제약이 없었기 때문에 결정트리가 학습 데이터에 과적합되었다.

일부 이상치 데이터까지 분류하기 위해 분할이  자주 일어나서 결정 기준 경계가 매우 많아졌다.

이렇게 복잡한 모은 학습 데이터 세트의 특성과 약간만 다른 형태의 데이터 세트를 예측하면 예측 정확도가 떨어지게 된다.

이번에는 min_samples_leaf=6을 설정해 6개 이하의 데이터는 리프노드를 생성 할 수 있도록 리프노드 생성 규칙을 완화한 뒤 하이퍼samples_leaf=6을 설정해 6개 이하의 데이터는 리프노드를 생성 할 수 있도록 리프노드 생성 규칙을 완화한 뒤 하이퍼 파라미터를 변경해 어떻게 결정기준 경계가 변하는지 살펴보자.

```python
#min_samples_leaf=6으로 트리 생성 조건을 제약한 결정 경계 시각화
dt_clf=DecisionTreeClassifier(min_samples_leaf=6).fit(X_features,y_labels)
visualize_boundary(dt_clf,X_features,y_labels)
```

위의 코드에서 이 부분만 고쳐주면 된다.

![결과3](https://user-images.githubusercontent.com/77263283/124779315-a3ae9b00-df7c-11eb-98c0-ae0266f3d6bd.png)

옆의 결정 경계 그래프를 보면 이상치에 크게 반응하지 않으면서 좀 더 일반화된 분류 규칙에 따라 분류됐음을 알 수 있다.

다양한 테스트 데이터 세트를 기반으로 한 결정트리 모델의 예측 성능은 첫 번째 모델보다는 트리생성 조건을 제약한 두번째 모델이 더 뛰어날 가능성이 높다. 왜냐하면 테스트 데이터세트는 오히려 테스트 데이터 세트에서 정확도를 떨어뜨릴 수 있기 때문이다.
