## 신용카드 사기 검출 실습

캐글의 신용카드 데이터 세트를 이용해 신용카드 사기 검출 분류 실습을 해보자.

해당 데이터 세트의 레이블인 Calss속성은 매우 불균형한 분포를 가지고 있다. Class는 0과 1로 분류되는데 0이 사기가 아닌 정상적인 신용카드 트랜잭션 데이터, 1은 신용카드 사기 트랜잭션을 의미한다. 사기 트랜잭션의 비율은 전체 데이터의 약 0.172%뿐이다. 이런 불균형한 분포를 가진 데이터 세트는 오버 샘플링 또는 언더 샘플링으로 충분한 학습 데이터 값을 확보해야 한다.

오버 샘플링? 

이상 데이터와 같이 적은 데이터 세트를 증식하여 학습을 위한 충분한 데이터를 확보하는 방법이다. 원본 데이터의 피처 값들을 아주 약간만 변경하여 증식한다.

언더 샘플링?

많은 데이터 세트를 적은 데이터 세트 수준으로 감소 시키는 방식이다. 너무 많은 정상 레이블 데이터를 감소시키면 오히려 제대로 된 학습을 수행 할 수 없다.

데이터 세트를 로딩해보자.

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')
%matplotlib inline

card_df=pd.read_csv("/content/drive/MyDrive/Colab Notebooks/creditcard.csv")
card_df.head(3)
```

[output]

![결과1](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/72ff44fe-b2be-4954-9e37-0475cd312d2a/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAT73L2G45O3KS52Y5%2F20210610%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20210610T111054Z&X-Amz-Expires=86400&X-Amz-Signature=aa5063b77e95d5a562d23b7681a6319655f77287b95cc4bdd1247445e894f58c&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22)

....

![결과2](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/647ee7e1-37cf-40a2-9a65-71680cf56567/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAT73L2G45O3KS52Y5%2F20210610%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20210610T111131Z&X-Amz-Expires=86400&X-Amz-Signature=405cc3c63dfc0e15bbb4ad558afdb6cb01cec793251923380a2b59695bc5a797&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22)

creditcard.csv의 V로 시작하는 피처들의 의미는 알 수 없다. Time피처의 경우는 데이터 생성 관련한 작업용 속성으로서 큰 의미가 없기에 제거한다. Amount 피처는 신용카드 트랜잭션 금액을 의미하고, Class는 레이블 값이다. 

crad_df.info()로 널 값과 데이터 타입을 살펴보자.

```python
card_df.info()
```

[output]

![결과3](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/bccc6e42-8cfa-4940-bfb8-9b41717998c8/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAT73L2G45O3KS52Y5%2F20210610%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20210610T111201Z&X-Amz-Expires=86400&X-Amz-Signature=a2276bf89745e7fccd00ea4e04968300741dcd7afe8f56d62f1f27ecca439787&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22)

null 값은 없으며 모두 숫자형 타입의 데이터이다.

이번 실습에서는 다양한 데이터 사전 가공을 수행하기 위해서 데이터를 가공하고 반환해주는 get_train_test_df() 함수를 생성하자. 일단 Time피처의 값만 삭제하는 것부터 시작하자.

```python
from sklearn.model_selection import train_test_split

#인자로 입력받은 DataFrame을 복사 한 뒤 Time 컬럼만 삭제하고 복사된 DataFrame 반환
def get_preprocessed_df(df=None):
  df_copy=df.copy()
  df_copy.drop('Time',axis=1,inplace=True)
  return df_copy
```

또한 학습 데이터 세트와 테스트 데이터 세트를 반환해주는 get_train_test_dataset()함수를 정의하자.

```python
#사전 데이터 가공 후 학습과 테스트 데이터 세트를 반환하는 함수
def get_train_test_dataset(df=None):
  #인자로 입력된 DataFrame의 사전 데이터 가공이 완료된 복사 DataFrame 반환
  df_copy=get_preprocessed_df(df)
  #DataFrame의 맨 마지막 칼럼이 레이블, 나머지는 피처들
  X_features=df_copy.iloc[:,:-1]
  y_target=df_copy.iloc[:,-1]
  #학습과 테스트 데이터 분할. stratify=y_target으로 Stratified 기반 분할
  X_train,X_test,y_train,y_test=train_test_split(X_features,y_target,test_size=0.3,random_state=0,stratify=y_target)
  #학습과 테스트 데이터 세트 반환
  return X_train,X_test,y_train,y_test

X_train,X_test,y_train,y_test=get_train_test_dataset(card_df)
```

다음으로 성능을 평가해줄 get_clf_eval()함수를 정의하자.

```python
from sklearn.metrics import confusion_matrix,accuracy_score,precision_score,recall_score,roc_auc_score,f1_score
def get_clf_eval(y_test, pred=None,pred_proba=None):
  confusion=confusion_matrix(y_test,pred)
  accuracy=accuracy_score(y_test,pred)
  precision=precision_score(y_test,pred)
  recall=recall_score(y_test,pred)
  f1=f1_score(y_test,pred)

  #ROC-AUC 추가
  roc_auc=roc_auc_score(y_test,pred_proba)
  print('오차 행렬')
  print(confusion)
  #ROC-AUC print 추가
  print('정확도:{0:.4f},정밀도{1:.4f}, 재현율:{2:.4f}, F1:{3:.4f}, AUC:{4:.4f}'.format(accuracy,precision,recall,f1,roc_auc))
```

이제 로지스틱 회귀와 LightGBM 기반의 모델을 수행하며 예측 성능이 어떻게 변하는지 보자.

```python
from sklearn.linear_model import LogisticRegression
from lightgbm import LGBMClassifier

#인자로 사이킷런의 Estimator객체와 학습/테스트 데이터 세트를 입력 받아 학습/예측/평가 수행
def get_moddel_train_eval(model,ftr_train=None,ftr_test=None,tgt_train=None,tgt_test=None):
  model.fit(ftr_train,tgt_train)
  pred=model.predict(ftr_test)
  pred_proba=model.predict_proba(ftr_test)[:,1]
  print(model.__class__.__name__)
  get_clf_eval(tgt_test,pred,pred_proba)

lg_clf=LogisticRegression()
get_moddel_train_eval(lg_clf,ftr_train=X_train,ftr_test=X_test,tgt_train=y_train,tgt_test=y_test)

lgbm_clf=LGBMClassifier(n_estimators=1000, num_leaves=64, boost_from_average=False)
get_moddel_train_eval(lgbm_clf,ftr_train=X_train,ftr_test=X_test,tgt_train=y_train,tgt_test=y_test)
```

[output]

![결과4](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/211ec821-12b9-4d35-a92d-0c6982220361/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAT73L2G45O3KS52Y5%2F20210610%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20210610T111222Z&X-Amz-Expires=86400&X-Amz-Signature=9e3080b82c5c8283238a0d7970549251c95648342c8ed431ebc4639bb35940e2&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22)

LigtGBM이 LogisticRegression보다 좋은 재현율과 ROC-AUC수치를 나타내었다.

이번에는 왜곡된 분포도를 가지는 데이터를 재가공한 뒤에 다시 모델을 테스트해보자.

먼저 creditcard.csv의 중요 피처 값의 분포도를 살펴보자. 로지스틱 회귀는 선형 모델인데, 선형 모델은 중요 피처값들이 정규 분포 형태를 따르는 것을 선호한다. 

Amount 피처는 신용카드 사기 금액으로 정상/사기 트랜잭션을 결정하는 매우 중요한 속성일 가능성이 높다. Amount 피처의 분포도를 확인해 보자.

```python
import seaborn as sns
plt.figure(figsize=(8,4))
plt.xticks(range(0,30000,1000),rotation=60)
sns.distplot(card_df['Amount'])
```

[output]

![결과5](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/14ef6c8f-a241-4d3d-a580-aab4c8730e2d/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAT73L2G45O3KS52Y5%2F20210610%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20210610T111309Z&X-Amz-Expires=86400&X-Amz-Signature=7f820f4ddb538e2cd01afed239f43d466912cad658e53cfc4badd3c539d6ee70&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22)

카드 사용금액이 100불 이하는 데이터가 대부분이며, 27,000불까지 드물지만 많은 금액을 사용한 경우가 발생하면서 꼬리가 긴 형태의 분포 곡선을 가지고 있다. Amount를 표준 정규 분포 형태로 변환한 뒤에 로지스틱 회귀의 예측 성능을 측정해 보자

```python
from sklearn.preprocessing import StandardScaler
#사이킷런의 StandardScaler를 이용해 정규 분포 형태로 Amount 피처값 변환하는 로직으로 수정
def get_preprocessed_df(df=None):
  df_copy=df.copy()
  scaler=StandardScaler()
  amount_n=scaler.fit_transform(df_copy['Amount'].values.reshape(-1,1))
  #변환된 Amount를 Amount_Scaled로 피처명 변경후 DataFrame맨 앞 컬럼으로 입력
  df_copy.insert(0,'Amount_Scaled',amount_n)
  df_copy.drop(['Time','Amount'],axis=1,inplace=True)
  return df_copy

#Amount를 정규 분포 형태로 변환 후 로지스틱 회귀 및 LightGBM 수행
X_train,X_test,y_train,y_test=get_train_test_dataset(card_df)

lr_clf=LogisticRegression()
get_moddel_train_eval(lr_clf,ftr_train=X_train,ftr_test=X_test,tgt_train=y_train,tgt_test=y_test)

lgbm_clf=LGBMClassifier(n_estimators=1000, num_leaves=64)
get_moddel_train_eval(lgbm_clf,ftr_train=X_train,ftr_test=X_test,tgt_train=y_train,tgt_test=y_test)
```

[output]

![결과6](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/a4f1b57b-169b-4c4b-b81c-825e26463795/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAT73L2G45O3KS52Y5%2F20210610%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20210610T111347Z&X-Amz-Expires=86400&X-Amz-Signature=45973c43856257470f6305d0a2fa7415d9ee04033cf49c09c42b6b6377af0ecd&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22)

정규 분포 형태로 Amount 피처값을 변환한 후 데스트 데이터 세트에 적용한 로지스틱 회귀 및 LightGBM 두 모델 모두 변환 이전과 비교해 성능이 크게 개선되지는 않았다.

이번에는 StandardScaler가 아닌 로그 변환을 해보자. 로그 변환은 데이터 분포가 심하게 왜곡되어 있을 경우 적용하는 중요 기법 중에 하나이다.

```python
from sklearn.preprocessing import StandardScaler
#사이킷런의 StandardScaler를 이용해 정규 분포 형태로 Amount 피처값 변환하는 로직으로 수정
def get_preprocessed_df(df=None):
  df_copy=df.copy()
  #넘파이의 log1p()를 이용해 Amount를 로그 변환
  amount_n=np.log1p(df_copy['Amount'])
  df_copy.insert(0,'Amount_Scaled',amount_n)
  df_copy.drop(['Time','Amount'],axis=1,inplace=True)
  return df_copy

#Amount를 정규 분포 형태로 변환 후 로지스틱 회귀 및 LightGBM 수행
X_train,X_test,y_train,y_test=get_train_test_dataset(card_df)

lr_clf=LogisticRegression()
get_moddel_train_eval(lr_clf,ftr_train=X_train,ftr_test=X_test,tgt_train=y_train,tgt_test=y_test)

lgbm_clf=LGBMClassifier(n_estimators=1000, num_leaves=64, boost_from_average=False)
get_moddel_train_eval(lgbm_clf,ftr_train=X_train,ftr_test=X_test,tgt_train=y_train,tgt_test=y_test)
```

[output]

![결과7](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/bbc5c394-321f-4c35-beb7-7119446265d6/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAT73L2G45O3KS52Y5%2F20210610%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20210610T111408Z&X-Amz-Expires=86400&X-Amz-Signature=fe26e3c9008fa60f7534e379b8e651c9745dac2dd589c75acb06d24d1fc97abb&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22)

두 모델 모두 정밀도, 재현율, AOC_AUC에서 약간씩 성능이 개선되었음을 알 수 있다.

이번에는 이상치 데이터를 제거한 후 모델을 학습하고 평가해보자.

이상치 데이터는 전체 데이터의 패턴에서 벗어난 이상 값을 가진 데이터이며, 아웃라이어라고도 불린다. 이런 이상치로 인해 머신러닝 모델의 성능에 영향을 받는 경우가 발생하기 쉽다. 이상치 데이터는 IQR 방식을 이용해 제거 할 수 있는데, 이는 데이터가 가질 수 있는 최솟값과 최댓값을 정해 이 범위를 벗어나는 데이터를 이상치로 간주하는 방법이다.

IQR을 이용해서 이상치 데이터를 검출해보자. 매우 많은 피처가 있을 경우에는 레이블 값과 가장 상관성이 높은 피처들을 위주로 이상치를 검출하는 것이 좋다. 피처들의 상관관계를 살펴보자.

```python
import seaborn as sns

plt.figure(figsize=(9,9))
corr=card_df.corr()
sns.heatmap(corr,cmap='RdBu');
```

[output]

![결과8](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/41190eb5-d874-4f1a-b87e-5ffd6e15e79e/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAT73L2G45O3KS52Y5%2F20210610%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20210610T111427Z&X-Amz-Expires=86400&X-Amz-Signature=fc74000f3452b3d4c0e577194758a81d5924253013b35674eed3e285b045d308&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22)

결과를 보면 V14와  V17이 Class피처와 음의 상관관계가 높다고 나왔다. 이 중 V14에 대해서만 이상치를 찾아서 제거해보자. IQR을 이용해 이상치를 검추라는 함수를 생성한 뒤, 이를 이용해 검출된 이상치를 삭제한다.

```python
import numpy as np

def get_outlier(df=None, column=None, weight=1.5):
  #fraud에 해당하는 column데이터만 추출, 1/4분위와 3/4분위 지점을 np.percentile로 구함
  fraud=df[df['Class']==1][column]
  quantile_25=np.percentile(fraud.values,25)
  quantile_75=np.percentile(fraud.values,75)
  #IQR을 구하고, IQR에 1.5를 곱해 최댓값과 최솟값 지점 구함
  iqr=quantile_75-quantile_25
  iqr_weight=iqr*weight
  lowest_val=quantile_25-iqr_weight
  highest_val=quantile_75+iqr_weight
  #최댓값보다 크거나, 최솟값보다 작은 값을 이상치 데이터로 설정하고 DataFrame index반환.
  outlier_index=fraud[(fraud<lowest_val) | (fraud > highest_val)].index
  return outlier_index

outlier_index= get_outlier(df=card_df, column='V14', weight=1.5)
  print('이상치 데이터 인덱스:',outlier_index)
```

[output]

![결과9](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/47530ee9-9b74-473a-80d7-95f8651a7b6c/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAT73L2G45O3KS52Y5%2F20210610%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20210610T111449Z&X-Amz-Expires=86400&X-Amz-Signature=f55bcffff03f0dfa5ac8cb459a255e4d588ddacb8d0dba9635519c4121003c55&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22)

이상치 데이터는 총 4개이다. get_outlier()를 이용해 이상치를 추출하고 이를 삭제하는 로직을 get_processed_df()함수에 추가해 데이터를 가공한 뒤 이를 데이터 세트를 이용해 로지스틱 회귀와 LightGBM모델을 다시 적용해 보자.

```python
#get_processed를 로그 변환 후 V14 피처의 이상치 데이터를 삭제하는 로직으로 변경
def get_preprocessed_df(df=None):
  df_copy=df.copy()
  amount_n=np.log1p(df_copy['Amount'])
  df_copy.insert(0,'Amount_Scaled',amount_n)
  df_copy.drop(['Time','Amount'],axis=1,inplace=True)
  #이상치 데이터를 삭제하는 로직 추가
  outlier_index=get_outlier(df=card_df, column='V14', weight=1.5)
  df_copy.drop(outlier_index,axis=0, inplace=True)
  return df_copy

X_train,X_test,y_train,y_test=get_train_test_dataset(card_df)
print("### 로지스틱 회귀 예측 성능###")
lr_clf=LogisticRegression()
get_moddel_train_eval(lr_clf,ftr_train=X_train,ftr_test=X_test,tgt_train=y_train,tgt_test=y_test)

print("### LightGBM 예측 성능###")

lgbm_clf=LGBMClassifier(n_estimators=1000, num_leaves=64, boost_from_average=False)
get_moddel_train_eval(lgbm_clf,ftr_train=X_train,ftr_test=X_test,tgt_train=y_train,tgt_test=y_test)
```

[output]

![결과10](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/9aca43fa-289c-43d7-ae17-5f7f2a0d852b/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAT73L2G45O3KS52Y5%2F20210610%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20210610T111504Z&X-Amz-Expires=86400&X-Amz-Signature=0b43fc6fa8434dd4758b2b91523aacd13d9d53e817681cbe8b15436e82ddff1d&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22)

이상치 데이터를 제거한 뒤, 로지스틱 회귀와 LightGBM 모두 예측 성능이 크게 향상되었다.

이번에는 SMOTE기법으로 오버 샘플링을 적용항 뒤 로지스틱 회귀와 LightGBM 모델의 예측 성능을 평가해 보자. 앞에서 생성한 학습 피처/레이블 데이터를 SMOTE 객페의 fit_sample() 메서드를 이용해 증식 한 뒤 데이터를 증식 전과 비교해 보자.

 

```python
from imblearn.over_sampling import SMOTE

smote=SMOTE(random_state=0)
X_train_over,y_train_over=smote.fit_sample(X_train,y_train)
print('SMOTE 적용 전 학습용 피처/레이블 데이터 세트:',X_train.shape,y_train.shape)
print('SMOTE 적용 후 학습용 피처/레이블 데이터 세트:',X_train_over.shape,y_train_over.shape)
print('SMOTE 적용 후 레이블 값 분포\n',pd.Series(y_train_over).value_counts())
```

[output]

![결과11](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/89001d5c-cb29-461e-8ade-32c56356038e/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAT73L2G45O3KS52Y5%2F20210610%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20210610T111632Z&X-Amz-Expires=86400&X-Amz-Signature=954275488652c41861b925ad31228f5234f1d99161a2cedadeaef7ec51a16dc0&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22)

SMOTE 적용 전 학습 데이터 세트는 199,362건이었지만 SMOTE 적용 후 2배 가까운 398,040건으로 데이터가 증식 되었다. 그리고 SMOTE 적용 후 레이블 값이 0과 1의 분포가 동일하게 199,020건으로 생성되었다.

이제 이렇게 생성된 학습 데이터 세트를 기반으로 먼저 로지스틱 회귀 모델을 학습한 뒤 성능을 평가해 보자.

```python
lr_clf=LogisticRegression()
#ftr_train과 tgt_train 인자값이 SMOTE 증식된 X_train_over, y_train_over로 변경됨에 유의
get_moddel_train_eval(lr_clf,ftr_train=X_train_over,ftr_test=X_test,tgt_train=y_train_over,tgt_test=y_test)
```

[output]

![결과12](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/0066a324-0cf3-4f1a-86f7-1128c6e05dec/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAT73L2G45O3KS52Y5%2F20210610%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20210610T111652Z&X-Amz-Expires=86400&X-Amz-Signature=7193795adf5db3552cec242254685b200d84bf9768406d7dee821b1d1b89faf2&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22)

로지스틱 회귀 모델의 경울 SMOTE로 오버 샘플링된 데이터로 학습할 경우 재현율이 92.47%로 크게 증가하지만, 반대로 정밀도는 5.42%로 급격하게 낮아진다. 이는 로지스틱 회귀 모델이 오버 샘플링으로 인해 실제 원본 데이터의 유형보다 너무나 많은 Class=1 데이터를 학습하면서 실제 데이터 세트에서 예측을 지나치게 Class=1로 적용해 정밀도가 급격하게 떨어지게 되것이다.

이번에는 LightGBM 모델을 SMOTE로 오버 샘플링된 데이터 세트로 학습/예측/평가를 수행해보자.

```python
lgbm_clf=LGBMClassifier(n_estimators=1000,num_leaves=64,n_jobs=-1, boost_from_average=False)
get_moddel_train_eval(lgbm_clf,ftr_train=X_train_over,ftr_test=X_test,tgt_train=y_train_over,tgt_test=y_test)
```

[output]

![결과13](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/efff9cf6-552d-45ea-860e-27d73cd3bea6/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAT73L2G45O3KS52Y5%2F20210610%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20210610T111710Z&X-Amz-Expires=86400&X-Amz-Signature=1923b844b176f9f5b4e9a9c028b8978d95265afa6941c91d285bfaf80cea9d94&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22)

재현율이 이상치만 제거한 경우린 82.88%보다 높은 84.93%가 되었다. 그러나 정밀도는 이전의 96.8%보다 낮은 93.23%이다. SMOTE를 적용하면 재현율은 높아지나, 정밀도는 낮아지는 것이 일반적이다. 좋은 SMOTE 패키지 일수록 재현율 증가율은 높이고 감소율은 낮출 수 있도록 효과적으로 데이터를 증식한다.
