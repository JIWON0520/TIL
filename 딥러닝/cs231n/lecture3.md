## Lecture 3 - Loss Functions and Optimization

lecture2 에서 linear classifier가 학습데이터를 학습하고 그 정보를 요약하여 metrix W에 압축한다고 했다. 이번 lecture3에서는 linear classifier가 어떻게 파라미터 W를 선택하는지 알아보자.

![사진1](https://user-images.githubusercontent.com/77263283/131453317-54c592ea-49ad-473e-9c57-4c3f407e24be.png)

우리는 가장 최선인 W를 찾기 위한 정량적인 방법이 필요하다. 이 W를 가지고 W가 얼마나 좋은지 안좋은지를 판별해주는 함수를 **'손실(loss) 함수'** 라고 한다. 

위의 예제에서 classifier는 세개의 이미지를 분류했는데, 이 분류기는 잘 작동하고 있지 않다. 자동차 이미지를 보면 자동차 스코어는 4.9로 다른 카테고리 스코어보다 충분히 높기 때문에 잘분류 되었지만, 개구리 이미지의 경우 정답인 개구리 스코어는 -3.1로 다른 카테고리 스코어보다 월등히 낮다.

손실 함수를 얘기 할때 고려해야 할것은 x와 y이다. x는 input값으로 사용되는 학습 데이터 세트이고 y는 알고리즘이 에측하는 예측값인 output이다. 

위의 수식에서 L_i 은 함수 f로부터 나온 예측 스코어와 실제 target값을 사용해서 이 알고리즘이 잘 작동하는지 알려주는 정량화된 지표이다.  최종 손실함수는 이 개개의 학습 데이터의 L_i의 값을 더해 평균을 취한 값이다. 

딥러닝 알고리즘 뿐만아니라 다른 알고리즘에서도 적용되는 가장 일반적인 일은 metrix W가 좋은지 안좋은지를 알 수 있는 손실함수를 정의하고 W가 존재하는 모든 공간에서 손실함수를 최소화 할 수 있는 W를 찾는 것이다.

먼저 살펴볼 손실함수는 SVM 손실이다.

![사진2](https://user-images.githubusercontent.com/77263283/131453334-c8747a66-5b45-40ca-868f-759e0d8a4ddc.png)

손실 L_i는 target이 아닌 카테고리 스코어(S_j)와 target 카테고리 스코어(S_Y_i)를 비교하여 그 손실 값을 모두 더한값이다.  만약, target 카테고리 스코어 값이 다른 카테고리 스코어보다 크다면, 그리고 일정 마진 보다 크다면 그 손실값은 0이 될것이다. target 카테고리의 L_i값을 제외한 모든 L_i값을 더하면 그것이 그 샘플의 총 손실 값이 된다. 또한 각 샘플의 최종 손실 값을 더해 평균을 취하면 그것이 최종 손실값이 된다.

실제 연산은 어떻게 되는지 예제로 살펴보자.

![사진3](https://user-images.githubusercontent.com/77263283/131453358-faddd3f0-f307-41d8-8652-0d23443c8296.png)

위 사진처럼 세개의 이미지는 세개의 카테고리중 하나로 분류된다.

첫 번째 사진의 target 스코어는 3.2이다. 이 이미지의 손실 값을 구해보자. 먼저 target 카테고리를 제외한 모든 다른 카테고리 스코어와 target 카테고리 스코어를 비교해야한다. 자동차 카테고리의 스코어는 5.1이다. 따라서 자동차 카테고리의 손실값은 max(0,5.1-3.2+1)로 2.9가 된다. 다음으로 개구리 카테고리의 손실값을 구하면 max(0,-1.7-3.2+1)로 0이된다. 따라소 이 고양이 이미지에 대한 손실 값은 2.9+0으로 2.9가 된다.

![사진4](https://user-images.githubusercontent.com/77263283/131453394-c57a49ed-fdca-4502-acd6-03b370e361b1.png)

다음으로 두번째 이미지의 손실 값을 구해보자. 이 사진의 target 카테고리는 자동차이다. 따라서 다른 카테고리와 자동차 카테고리의 스코어를 비교하고 이 합이 두번째 이미지의 손실 값이 된다. 자동차 카테고리의 스코어는 다른 카테고리 스코어보다 매우 높으므로 이 이미지의 손실값은 0이 된다.

![사진5](https://user-images.githubusercontent.com/77263283/131692075-3293e269-9792-4a2c-a8a4-97e20481e604.png)

마지막으로 개구리 이미지를 살펴보자. target 카테고리 스코어는 -3.1로 다른 카테고리 스코어보다 매우 낮다. 따라서 다른 카테고리 스코어와 비교해보면 개구리 이미지의 손실 값은 12.9로 비교적 높은 값을 가진다.

이 classifier의 총 손실 값은 **각 학습데이터의 손실 값의 평균**이므로 (2.9+0+12.9)/3=5.27이 총 손실 값이 된다. 손실 값은 이 classifier가 얼마나 제대로 작동하고 있는지 알려주는 정량적인 방법이다.

여기서 SVM loss에대한 몇가지 질문이 있다.

![사진6](https://user-images.githubusercontent.com/77263283/131692119-d4b9143d-5043-4684-a610-6930a3834621.png)

첫 번째는 만약 자동차이미지의 스코어를 조금 바꾼다면 손실 값은 어떻게 될까?

정답은 손실값은 변함이 없다. SVM loss는 스코어의 절대적인 값에 관심을 두는 것이아니다 target 카테고리 스코어가 다른 카테고리 스코어보다 얼마나 높은지에 관심을 두기 때문에 자동차 이미지에서 각 카테고리 스코어를 조금 바꾼다고 해도 자동차 카테고리 스토어가 월등히 높기 때문에 여전히 loss값은 0일것이다.

![사진7](https://user-images.githubusercontent.com/77263283/131692146-03938b24-b68e-49d5-9f0f-d42312833d59.png)

두 번째 질문은 loss 값의 최대와 최소는 무엇일까?

loss값의 최소는 0이고 최대는 무한대이다. loss 값이 최소라는건 모든 이미지에 대한 분류가 정확했다는 것이다. 따라서 각 이미지의 target 카테고리 스코어는 다른 카테고리 스코어보다 높을 것이므로 loss값은 0이된다. loss 값의 최대는 무한대이다. 위의 손실 함수의 그래프를 보면 target 카테고리 스코어가 다른 카테고리 스코어보다 매우 낮으면 loss값은 엄청 커질것이고 그렇게 된다면 loss값의 최대는 무한대가 될것이다.

![사진8](https://user-images.githubusercontent.com/77263283/131832509-35b7c515-e096-42fc-88c6-aa0093d84f45.png)

세번째 질문은 만약  W가 아주 작은 값으로 초기화 되어서 모든 카테고리 스코어가 0에 가까워지면 손실값은 어떻게 될까?

정답은 손실값은 '(카테고리 개수)-1'이 된다. 모든 카테고리 스코어가 0이 되면 max(0,S_j - S_Y_i+1)=1이 되고 정답이 아닌 모든 카테고리를 루프하여 비교하므로 그 값을 더하게 되면 총 손실 값은  '(카테고리 개수)-1'이 된다.

![사진9](https://user-images.githubusercontent.com/77263283/131832528-6e5afd94-dd50-4fc6-839d-fd5991665073.png)

다음 질문은 만약 모든 카테고리를 더하여 손실 값을 구한다면 어떻게 될까?

정답은 loss 값+1이된다. 만약 target카테고리까지 고려하여 루프를 돈다면 target 카테고리의 loss값은 1이므로 최종 loss 값은 원래 loss값+1이 될것이다. 우리가 정답 카테고리만 빼고 값을 더하는 이유는 loss 값이 0이 되어야 '아무것도 잃는 것이 없다'라고 쉽게 해석될 수 있기 때문에 관례상 정답 카테고리의 값은 빼고 계산하는 것이다. 정답 카테고리를 포함해서 계산한다고해도 다른 classifier가 학습되는 것은 아니다.

![사진10](https://user-images.githubusercontent.com/77263283/131832557-a056ceb9-4591-4fad-8812-ad59099591aa.png)

다음 질문은 만약 loss의 합 대신 평균을 쓰면 어떻게 될까?
정답은 아무런 영향이 없다는 것이다. 각 카테고리 개수는 우리가 데이터 세트를 선택할 때 이미 정해지는 것이다. 따라서 loss 함수를 평균을 취한다는 것은 단지 loss 함수를 리스케일링 한다는 것이다. 이것은 별다른 영향을 끼치지 않는데 그 이유는 우리는 정답 스코어의 절대적인 값에 신경을 쓰는 것이 아니기 때문이다.

![사진11](https://user-images.githubusercontent.com/77263283/131832575-f4aaf52d-c578-44b4-b3f0-b3fffe9832df.png)

다음 질문은 만약 max^2의 값을 쓰면 loss 함수는 어떻게 될까? 이때 학습되는 classifier는 같은 것일까 다른 것일까?

정답은 다른 classifier가 학습될것이다. 제곱항을 쓰게 되면 좋은것과 나쁜것의 trade-off가 비선형적으로 바뀌게 되기때문에 결국 다른 loss 함수를 계산할 것이다. 이렇게 제곱 loss 함수는 종종 쓰인다. 

이렇게 SVM loss를 이해하기 위한 질문들을 해보았다.

근데 만약 loss를 0으로 만드는 W값을 찾았다고 할때, 이 W 값은 유일할가? 아님 다른 W도 있을 수 있을까? 정답은 다른 W값을 찾을 수 있다는 것이다. 모든 스코어는 W에 기반하여 스케일링 된다. 따라서 W가 두배가 된다고하면 정답 스코어와 정답이 아닌 스코어의 마진도 역시 두 배가 된다. 따라서 이미 마진이 1보다 크면 W를 두배한다해도 마진은 1보다 클것이기 때문에 loss값은 여전히 0일 것이다.

그렇다면 많은 W중에서 classifier는 어떻게 적합한 W를 찾을까? 만약 loss값이 0이 되는 W를 선택한다면 학습 데이터 세트에서는 잘 작동할 수 있지만 테스트 데이터에서는 그렇지 않을 수 있다. 머신러인의 목적은 테스트데이터에 작 적용되는 classifier를 찾는 것이기 때문에 학습데이터에만 잘 작동하는 즉, loss 값이 0이되는 W를 선택하는 것은 좋은것만은 아니다. 

![사진12](https://user-images.githubusercontent.com/77263283/131832590-90d8c5a9-21ce-45eb-9d2b-c08f86fb5bff.png)

위의 예시처럼 파란 점은 학습데이터라고 하자. loss 값이 0이되기 하기위해 classifier는 저렇게 학습 데이터 딱 맞춰진 구불구불한 선을 학습한다. 

![사진13](https://user-images.githubusercontent.com/77263283/131832605-f4e8ac6f-8684-4964-9501-2c848831dd32.png)

만약 위의 사진처럼 초록색 점으로 표현된 새로운 데이터가 들어오게 된다면 학습했던 구불구불한 파란선은 테스트 데이터의 예측을 실패하게 된다. 우리는 저 초록색 라인을 원할 것이다.

![사진14](https://user-images.githubusercontent.com/77263283/131832619-b319019c-197e-431f-8b6b-65f50074444f.png)

우리는 이런 경우를 막기위해 '규제'라는 것을 적용한다. 손실 함수에 규제에 해당하는 항(R이라고 쓰고 불림)을 더하는 것이다. 이 항을 더함으로써 우리는 classifier가 더 단순한 W를 찾도록 하는 것이다. 

이러한 아이디어는 '오컴의 면도날'이라는 아이디어에서 착안했는데 '오컴의 면도날'은 '많은 가설이 존재하면 더 단순한 것을 선택해라. 그것이 미래에 일어날 일을 잘 설명할 것이다.'라는 이야기이다. 이제 손실함수는 data항과 규제항이 존재하고 하이퍼파라미터 람다는 두 항간의 trade-off를 나타낸다.

![사진 15](https://user-images.githubusercontent.com/77263283/131832631-b5aa189b-e261-48a6-b05c-2124011a9d22.png)

규제의 종류에는 많은 것이 있지만 L2,L1 규제만 살펴보자. L2규제는 유클리디안 규제를 W에 가하는 것이고 L1규제는 matrix W를 희소 행렬로 만드는 것이다. 

![사진16](https://user-images.githubusercontent.com/77263283/131832652-aa6d06e5-422b-465e-922e-e3fcafff3695.png)

만약 [1,1,1,1]형태를 가지는 x가 있고 w1,w2 두가지의 W행렬이 있다고 가정하자. 두개의 W중 L2규제는 무엇을 더 선호할까?

답은 w2이다. 왜냐하면 w2가 더 적은 L2 norm를 가지기 때문이다. 두가지 matrix와 x를 내적한 결과값은 같을지라도 L2규제는 w2를 더 선호한다. matrix W의 각 요소는 x의 각 요소가 output값에 얼마나 영향이 있는지를 나타낸다. L2규제는 결과값이 x의 요소중 하나의 요소에 좌지우지 되는것보다 x의 각 요소에게 골고루 영향받는 것을 원한다. 

L1규제는 정반대이다. L1 규제는 모델의 복잡도를 W 행렬속 0의 갯수에따라 평가한다. 

이러한 규제방법들을 선택하는 기준은 우리의 데이터에 따라 달라진다. 

지금까지 우리는 SVM loss에 대해 살펴보았다. SVM외에도 머신러닝에서 인기이는 loss 함수가 있는데, 그것은 '다항 로지스틱 회귀' 또는 'softmax loss'이다. SVM loss에서는 각 카테고리 스코어에대한 의미를 생각하지 않고 단지 정답 스코어와 그외 스코어의 차이에 중점을 둔 반면, 다항 회귀 손실함수는 각 스코어에 추가적인 의미를 부여한다. 특히 이 스코어로 각 카테고리에 대한 확률분포를 계산한다.

![사진17](https://user-images.githubusercontent.com/77263283/131832664-c9eecbfe-bcbf-436c-bcce-c740b6ebce99.png)

우리는 여기서 'softmax'라 불리는 함수를 쓸것이다. 먼저 각 스코어에 지수를 취한다. 그럼 모두 양수가 되는데 이를 다시 각 지수값의 합으로 정규화한다. 그럼 0~1사이의 각 카테고리에대한 확률값이 나오는데 이 확률값을 모두 더하면 1이 된다.  궁극적으로 우리는 이 소프트맥스 함수를 거친 정답 카테고리의 확률값이 1에 가깝게 되기를 원할것이다.

우리의 손실함수는 -logP가 될것이다. 왜냐하면 우리는 P가 1에 최대한 가깝게 되길 원하는데 단지 P를 최대화하는것보다 log를 최대화하는것이 더 쉽기 때문이다. 그리고 우리는 손실함수가 '얼마나 좋은지'가 아닌 '얼마나 나쁜지'를 알고 싶으므로 앞에 -를 취해주는 것이다.

![사진18](https://user-images.githubusercontent.com/77263283/131832676-9bf52005-c0b7-4531-af69-8ade863c73a7.png)

위의 예시를 보자. 각 카테고리 스코어는 SVM loss의 예제와 같은 값이다. sofrmax에서는 우선 이 스코어들에 지수를 취해주고 다시 모든 합이 1이되게 정규화를 시켜준다. 그럼 정답 카데고리의 확률값은 0.13이되는게, 여기서 손실 값은 -log(0.13)이된다.
